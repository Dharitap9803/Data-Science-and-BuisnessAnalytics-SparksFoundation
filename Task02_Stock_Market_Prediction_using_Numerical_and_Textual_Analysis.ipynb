{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "GRIP internship #July2024\n",
        "Task2 : #7 STOCK MARKET PREDICTION USING NUMERICAL AND TEXTUAL ANALYSIS\n",
        "Task :\n",
        "\n",
        "Create a hybrid model for stock price/performance prediction using numerical analysis of historical stock prices, and sentimental analysis of news headlines\n",
        "By Dharita Patel\n",
        "In this I have predicted if a companies stock will increase or decrease based on news headlines using sentiment analysis.\n",
        "\n",
        "This model will determine if the price of a stock will increase or decrease based on the sentiment of top news article headlines for the current day using Python and machine learning.\n",
        "\n",
        "I have used both numerical and textual data for this.\n",
        "\n",
        "(i) Time series analysis is performed on the Stock data.\n",
        "\n",
        "(ii) Sentiment analysis is performed on the News data.\n",
        "\n",
        "(iii) An analysis is performed by merging both the data to predict if the Close price of the stock will increase or decrease.\n"
      ],
      "metadata": {
        "id": "yNNRo4w4UTPj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas-datareader\n",
        "!pip install pmdarima\n",
        "! pip install textBlob\n",
        "! pip install vaderSentiment"
      ],
      "metadata": {
        "id": "VU1WEDKKUe5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "IMPORTING ALL THE NECESSARY LIBRARIRES\n"
      ],
      "metadata": {
        "id": "yXGIseXJVQXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pylab as pl\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "import datetime\n",
        "import pandas_datareader.data as web\n",
        "import matplotlib as mpl\n",
        "from matplotlib import style\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.filterwarnings('ignore')\n",
        "import nltk\n",
        "from statsmodels.tsa.stattools import adfuller, acf, pacf\n",
        "from statsmodels.tsa.arima_model import ARIMA\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error,r2_score\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('vader_lexicon')\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "from textblob import TextBlob\n",
        "from matplotlib.pyplot import figure\n",
        "from matplotlib import rcParams\n",
        "import plotly.express as px\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\n",
        "import xgboost\n",
        "import lightgbm"
      ],
      "metadata": {
        "id": "NfrGQA-LVPkC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing stocks data from web"
      ],
      "metadata": {
        "id": "YY276-JtV-a8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = datetime.datetime(2010, 1, 1)\n",
        "end = datetime.datetime.today()"
      ],
      "metadata": {
        "id": "l83CcAU-V5Pp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "!pip install yfinance\n",
        "import yfinance as yf\n",
        "\n",
        "# Use yfinance to fetch data\n",
        "stocks = yf.download(\"AAPL\", start=start, end=end)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "tJJFcVzXWK4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stocks.info()"
      ],
      "metadata": {
        "id": "NRmPMouTWWkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "2. Load the Textual Data"
      ],
      "metadata": {
        "id": "ti0k1Gv4WxcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stocks.head()"
      ],
      "metadata": {
        "id": "SR-ACS0WWwtb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stocks.reset_index(inplace=True)\n",
        "stocks.tail()"
      ],
      "metadata": {
        "id": "W8hXzOSggusr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stocks.shape"
      ],
      "metadata": {
        "id": "aXTcUWKZg0mH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stocks.describe()"
      ],
      "metadata": {
        "id": "j18g2nmpg4T3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Cleaning"
      ],
      "metadata": {
        "id": "Bapt5gj2g6zM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stocks['Date'] = pd.to_datetime(stocks['Date'])\n",
        "stocks.head()"
      ],
      "metadata": {
        "id": "Mxc4RkABg91M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Missing Values"
      ],
      "metadata": {
        "id": "sCsUI51-hCH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stocks.isna().sum()"
      ],
      "metadata": {
        "id": "XXEL2j2shEEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizing the Data-Closing Data"
      ],
      "metadata": {
        "id": "zhCCJVEJhPge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(5,5))\n",
        "plt.title('Closing Price of Stocks', fontsize = 18)\n",
        "plt.xlabel('Days', fontsize= 18)\n",
        "plt.ylabel('Close', fontsize = 18)\n",
        "plt.plot(stocks['Close'])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oZqwRnj8hJCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizing the Data-Open Price\n"
      ],
      "metadata": {
        "id": "ORSsDhdVhVhX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(5,5))\n",
        "plt.grid(True)\n",
        "plt.plot(stocks['Open'])\n",
        "plt.xlabel('Days')\n",
        "plt.ylabel('Open Price')\n",
        "plt.title('Opening price of Stocks')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "T41z63SohYhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizing the stock return"
      ],
      "metadata": {
        "id": "CR7IYmi8hdMh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "close = stocks['Close']\n",
        "returns = close / close.shift(1) - 1\n",
        "\n",
        "plt.figure(figsize = (10,6))\n",
        "returns.plot(label='Return', color = 'g')\n",
        "plt.title(\"Stock Returns\")"
      ],
      "metadata": {
        "id": "MzAL4vG3hfmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time series Analysis -For Close Price\n",
        "\n",
        "We can also perform the same analysis for Open price as well\n",
        "Time series decomposition involves thinking of a series as a combination of level, trend, seasonality, and noise components. we need to separate seasonality and trend from our series. The resultant series will become stationary through this process.\n",
        "Splitting data into train and test data"
      ],
      "metadata": {
        "id": "z5xp-8KAhlGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = stocks[:1600]\n",
        "test = stocks[1600:]"
      ],
      "metadata": {
        "id": "5Vaj08SMhmIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.shape, test.shape"
      ],
      "metadata": {
        "id": "hBnBKqrMhq4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decomposition of Time series\n",
        "Stationarity\n",
        "A stationary process has a mean and variance that do not change overtime and the process does not have trend.\n",
        "\n",
        "The above time series does not look stationary.\n",
        "\n",
        "To confirm that we will use “Dickey-Fuller test” to determine stationarity.\n",
        "\n",
        "Dickey-Fuller test for variable\n",
        "Dickey-Fuller test"
      ],
      "metadata": {
        "id": "eq_2PwNjhtz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def adfullerTest(X):\n",
        "    result = adfuller(X,autolag = 'AIC')\n",
        "    print('ADF Statistic: %f' % result[0])\n",
        "    print('p-value: %f' % result[1])\n",
        "    print('No of Lags Used: %f' % result[2])\n",
        "    print('Number of Obs Used: %f' % result[3])\n",
        "    print('Critical Values:')\n",
        "    for key, value in result[4].items():\n",
        "        print('\\t%s: %.3f' % (key, value))\n",
        "    if result[1] <=0.05 :\n",
        "         print(\"Reject against the null hypothesis, time series is stationary\")\n",
        "    else:\n",
        "        print(\"Accept null hypothesis, time series is non-stationary \")"
      ],
      "metadata": {
        "id": "yj2ukFWehuoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adfullerTest(train['Close'])"
      ],
      "metadata": {
        "id": "4eDDkMqAhzy7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rolling Statistics"
      ],
      "metadata": {
        "id": "Okms3bXUh2QO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rolling_mean_50 = (train['Close']).rolling(window=50).mean()\n",
        "rolling_std_50 = (train['Close']).rolling(window=50).std()\n",
        "plt.figure(figsize = (8,8))\n",
        "plt.plot((train['Close']), color = 'blue', label = 'original')\n",
        "plt.plot(rolling_mean_50, color = 'red', label = 'rolling mean 50')\n",
        "plt.plot(rolling_std_50, color = 'black', label = 'rolling std 50')\n",
        "plt.xlabel('Time')\n",
        "plt.legend()\n",
        "plt.title('Mean and Standard Deviation on  transformed data',  fontsize = 20)"
      ],
      "metadata": {
        "id": "-kwwMc7Hh35f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the data shows changing variance over time, the first thing we will do is stabilize the variance by applying log transformation using the log() function. The resulting series will be a linear time series.\n",
        "Log Transfromation\n",
        "Let’s log transform the dataset again to make the distribution of values more linear and better meet the expectations of this statistical test."
      ],
      "metadata": {
        "id": "vrD4RWLIh_fc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_log = np.log(train['Close'])\n",
        "test_log = np.log(test['Close'])"
      ],
      "metadata": {
        "id": "sc5tiKobiDs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import log\n",
        "adfullerTest(log(train['Close']))"
      ],
      "metadata": {
        "id": "WIVikGuhiAV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the value is larger than the critical values, again, meaning that we can fail to reject the null hypothesis and in turn that the time series is non-stationary."
      ],
      "metadata": {
        "id": "GIVAd0S6iK1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rolling_mean_50 = log(train['Close']).rolling(window=50).mean()\n",
        "rolling_std_50 = log(train['Close']).rolling(window=50).std()\n",
        "plt.figure(figsize = (8,8))\n",
        "plt.plot(log(train['Close']), color = 'g', label = 'Log Transformed')\n",
        "plt.plot(rolling_mean_50, color = 'r', label = 'rolling mean 50')\n",
        "plt.plot(rolling_std_50, color = 'r', label = 'rolling std 50')\n",
        "plt.xlabel('Time')\n",
        "plt.legend()\n",
        "plt.title('Mean and Standard Deviation on Log transformed data',  fontsize = 20)"
      ],
      "metadata": {
        "id": "W3fjjY6siLPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To Removing Linear Trend\n",
        "We will now perform the first difference transformation to our series to remove the linear trend.\n"
      ],
      "metadata": {
        "id": "a6HmAZXiiRtS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean_log = log(train['Close']).rolling(50).mean()"
      ],
      "metadata": {
        "id": "F3QjrXjWiSXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_log_diff = log(train['Close']) - mean_log\n",
        "train_log_diff.dropna(inplace = True)\n",
        "adfullerTest(train_log_diff)"
      ],
      "metadata": {
        "id": "zS4V3zbdiWwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=   train_log_diff\n",
        "mean = data.rolling(50).mean()\n",
        "std = data.rolling(50).std()\n",
        "plt.figure(figsize = (8,8))\n",
        "plt.plot(data, color = 'g', label = 'Differential Log Transformed data')\n",
        "plt.plot(mean, color = 'r', label = 'rolling mean')\n",
        "plt.plot(std, color = 'b', label = 'rolling std')\n",
        "plt.xlabel('Time')\n",
        "plt.legend()\n",
        "plt.title('Mean and Standard Deviation on Differential Log Transformed data',  fontsize = 20)"
      ],
      "metadata": {
        "id": "ISa39uTTibe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rejecting the null hypothesis means that the process has no unit root, and in turn that the time series is stationary or does not have time-dependent structure. Here we can also observe that there is no such trend in mean and Standard deviation So the now time series is statinary\n",
        "Now the data is stationary we can apply ARIMA model to our data"
      ],
      "metadata": {
        "id": "Rqz7dnS5ig-I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ARIMA Model\n",
        "AR: Autoregression. A model that uses the dependent relationship between an observation and some number of lagged observations.\n",
        "I: Integrated. The use of differencing of raw observations (e.g. subtracting an observation from an observation at the previous time step) in order to make the time series stationary.\n",
        "MA: Moving Average. A model that uses the dependency between an observation and a residual error from a moving average model applied to lagged observations.\n",
        "The parameters of the ARIMA model are defined as follows:\n",
        "\n",
        "p: The number of lag observations included in the model, also called the lag order.\n",
        "d: The number of times that the raw observations are differenced, also called the degree of differencing.\n",
        "q: The size of the moving average window, also called the order of moving average."
      ],
      "metadata": {
        "id": "VJ68DjGaik49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pmdarima as pmd\n",
        "\n",
        "def arimamodel(timeseriesarray):\n",
        "    autoarima_model = pmd.auto_arima(timeseriesarray,\n",
        "                             # start_p=1,\n",
        "                              #start_q=1,\n",
        "                              #test=\"adf\",\n",
        "                              trace=True,\n",
        "                              error_action = 'ignore',\n",
        "                              suppress_warnings = True)\n",
        "    return autoarima_model"
      ],
      "metadata": {
        "id": "qpsULMDlihqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stocks_arima = arimamodel((train_log))\n",
        "stocks_arima.summary()"
      ],
      "metadata": {
        "id": "PYN8B53Airsa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Warnings:\n",
        "[1] Covariance matrix calculated using the outer product of gradients (complex-step)."
      ],
      "metadata": {
        "id": "xnE1vUU6iv2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stocks_arima.plot_diagnostics(figsize=(10,10))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cMXnbFqMiwlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prediction on the Test Data"
      ],
      "metadata": {
        "id": "zwoz4Ueoi0uw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predict_ClosePrice = stocks_arima.predict(n_periods = len(test_log))\n",
        "predict_ClosePrice = pd.DataFrame(predict_ClosePrice,index = test_log.index,columns=['predict_ClosePrice'])"
      ],
      "metadata": {
        "id": "kL5J_gsBi3Yz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train_log, label='Train')\n",
        "plt.plot(test_log, label='Test')\n",
        "plt.plot(predict_ClosePrice, label='Prediction')\n",
        "plt.title(' Stock Price Prediction')\n",
        "plt.xlabel('Time')\n",
        "plt.ylabel('Actual Stock Price')"
      ],
      "metadata": {
        "id": "pSkg4jBwi8wl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Validation of the Arima Model"
      ],
      "metadata": {
        "id": "b6zyu14DjC8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('Mean Squared Error      ',mean_squared_error(test_log, predict_ClosePrice))\n",
        "print('Root Mean_Squared_Error ',np.sqrt(mean_squared_error(test_log, predict_ClosePrice)))\n",
        "print('Mean Absolute Error     ',mean_absolute_error(test_log, predict_ClosePrice))\n",
        "print('R-Squared               ',r2_score(test_log, predict_ClosePrice))\n"
      ],
      "metadata": {
        "id": "jhhVPUFyjAkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analyzing the newest Data"
      ],
      "metadata": {
        "id": "KQRYncdjjJeo"
      }
    },
    {
      "source": [
        "!pip install pandas\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Try to read the file, handling potential errors\n",
        "try:\n",
        "    news = pd.read_csv(\"/content/india-news-headlines.csv\")\n",
        "except pd.errors.ParserError as e:\n",
        "    print(f\"An error occurred while reading the file: {e}\")\n",
        "    # If the error is due to unterminated quotes, try:\n",
        "    news = pd.read_csv(\"/content/india-news-headlines.csv\", quoting=pd.QUOTE_NONE, error_bad_lines=False)\n",
        "    # Or, investigate the file around row 312461 for special characters or formatting issues."
      ],
      "cell_type": "code",
      "metadata": {
        "id": "XnXVlk2wCCue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news.head()"
      ],
      "metadata": {
        "id": "XJuTyE22jfOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Try to convert the 'publish_date' column to datetime, handling errors\n",
        "news['publish_date'] = pd.to_datetime(news['publish_date'], format='%Y%m%d', errors='coerce')\n",
        "\n",
        "# Check for invalid dates (NaT values)\n",
        "invalid_dates = news[news['publish_date'].isnull()]\n",
        "print(invalid_dates)\n",
        "\n",
        "# If you have invalid dates, investigate them further and decide how to handle them.\n",
        "# You might need to:\n",
        "# - Correct the format of those dates in the original data\n",
        "# - Remove those rows if they are not relevant\n",
        "# - Fill them with a default date if appropriate"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "pnYWHqHdjyeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news.shape"
      ],
      "metadata": {
        "id": "ZlWBiAfcj5ZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(news.columns)"
      ],
      "metadata": {
        "id": "2QydJFDCj8w7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analaysing \"HEADLINE_CATEGORY\" with \"CITIES\""
      ],
      "metadata": {
        "id": "WULHFqflkCKy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news['headline_category'].value_counts().head()"
      ],
      "metadata": {
        "id": "E0Im3-_gkC-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "# Handle missing values before applying str.contains\n",
        "news['headline_category'] = news['headline_category'].fillna('')  # Replace NaN with empty strings\n",
        "cities = news[news['headline_category'].str.contains('^city\\.[a-z]+', regex=True)]\n",
        "cities.head(10)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "sX5jTQhvkQRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "city = pd.DataFrame(columns = ['city_name'])\n",
        "city['city_name'] = cities.headline_category.str.split('.',expand = True)[1]\n",
        "cities = pd.concat([cities, city], axis = 1)\n",
        "cities.head()"
      ],
      "metadata": {
        "id": "ChhwHFIEkTLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cities.drop('headline_category', inplace =True,axis =1)"
      ],
      "metadata": {
        "id": "UK9eoHcLCR5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cities.head()"
      ],
      "metadata": {
        "id": "AaOXB8-cCUBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cites = cities.groupby(cities['city_name']).agg({'headline_text':'count'})\n",
        "cites.head()"
      ],
      "metadata": {
        "id": "qGex7Zs9CYms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cites.rename(columns = {'headline_text':'headline_count'}, inplace = True)\n",
        "cites = cites.sort_values(by='headline_count',ascending=False)\n",
        "cites.head()"
      ],
      "metadata": {
        "id": "MqdmGCAECcVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top10cites = cites.head(10)\n",
        "def fig_plot(top10cites,title1):\n",
        "    fig = px.line(top10cites,title =title1)\n",
        "    for i in top10cites.columns[0:]:\n",
        "        fig.add_bar(x= top10cites.index ,y = top10cites['headline_count'],name = i)\n",
        "    fig.show()\n",
        "fig_plot(top10cites,'Count of Headlines for top10 Cities')"
      ],
      "metadata": {
        "id": "vNGbPC_eCgZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cities.head()"
      ],
      "metadata": {
        "id": "wSC8MeIQCjtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analaysing \"HEADLINE_CATEGORY\" with \"CATEGORIES\""
      ],
      "metadata": {
        "id": "4AsxP9XOCnoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "news.head()"
      ],
      "metadata": {
        "id": "tlIccC9fCoUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news['category']=news['headline_category'].str.split('.').map(lambda x : x[0])"
      ],
      "metadata": {
        "id": "jFzldFB9CtE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categories = news.groupby(['category']).agg({'headline_text':'count'}).sort_values(by='headline_text',ascending = False)\n",
        "news_cat=categories.head(10)\n",
        "news_cat.reset_index(inplace = True)\n",
        "news_cat"
      ],
      "metadata": {
        "id": "9rXcrEJ9Cv_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.colors as mcolors\n",
        "plt.figure(figsize=(17,5))\n",
        "plt.bar(news_cat.category,height= news_cat.headline_text, color = 'c')\n",
        "plt.xlabel('Category')\n",
        "plt.ylabel('Number of articles')\n",
        "plt.title('Top 10 Categories')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HaHZ741nCzNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news.drop('headline_category', inplace  = True, axis =1)\n",
        "news.head()"
      ],
      "metadata": {
        "id": "x9FIDsvPC2LD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleaning the data -Removing Stop words"
      ],
      "metadata": {
        "id": "iXrvn31KC47u"
      }
    },
    {
      "source": [
        "# Convert all elements in 'headline_text' column to strings before joining\n",
        "headline_text = ' '.join(news['headline_text'].astype(str).str.lower())\n",
        "print(headline_text)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "7EO6rrmwDGQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "bBNIs1WmDYPG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordcloud = WordCloud(stopwords=stop_words, background_color=\"white\", max_words=1000).generate(headline_text)\n"
      ],
      "metadata": {
        "id": "Gs6Vi9E8DsGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rcParams['figure.figsize'] = 10, 10\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XZy4bpb8D_eC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentimental Analaysis -- Assigning Polarity to the Headlines"
      ],
      "metadata": {
        "id": "pNodCl-FELsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a function to get the subjectivity\n",
        "def Subjectivity(text):\n",
        "       return TextBlob(text).sentiment.subjectivity\n",
        "\n",
        "# Create a function to get the polarity\n",
        "def Polarity(text):\n",
        "      return  TextBlob(text).sentiment.polarity"
      ],
      "metadata": {
        "id": "PMHTthsWEILq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "# Function to get subjectivity\n",
        "def get_subjectivity(text):\n",
        "    return TextBlob(text).sentiment.subjectivity\n",
        "\n",
        "# Function to get polarity\n",
        "def get_polarity(text):\n",
        "    return TextBlob(text).sentiment.polarity\n",
        "\n",
        "# Apply the functions, converting non-string inputs to strings and handling NaN values\n",
        "news['Subjectivity'] = news['headline_text'].apply(lambda x: get_subjectivity(str(x)) if not pd.isna(x) else None)\n",
        "news['Polarity'] = news['headline_text'].apply(lambda x: get_polarity(str(x)) if not pd.isna(x) else None)\n",
        "\n",
        "# Initialize VADER sentiment analyzer\n",
        "senti = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Apply VADER sentiment analysis\n",
        "news['Compound'] = news['headline_text'].apply(lambda x: senti.polarity_scores(str(x))['compound'] if not pd.isna(x) else None)\n",
        "news['Negative'] = news['headline_text'].apply(lambda x: senti.polarity_scores(str(x))['neg'] if not pd.isna(x) else None)\n",
        "news['Neutral'] = news['headline_text'].apply(lambda x: senti.polarity_scores(str(x))['neu'] if not pd.isna(x) else None)\n",
        "news['Positive'] = news['headline_text'].apply(lambda x: senti.polarity_scores(str(x))['pos'] if not pd.isna(x) else None)\n"
      ],
      "metadata": {
        "id": "sfJ_osAyEIP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news.head()"
      ],
      "metadata": {
        "id": "V2ehZHmLEdK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hybrid model - Combining Stocks data and news data\n"
      ],
      "metadata": {
        "id": "6BqaPmdvJyB4"
      }
    },
    {
      "source": [
        "df_merge = pd.merge(stocks, news, how='inner', on='Date') # Replace 'hisdf' and 'ndf' with the actual DataFrame names you want to merge.\n",
        "df_merge"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "HNic7EzDHztj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dfmerge1 = df_merge[['Close','Subjectivity', 'Polarity', 'Compound', 'Negative', 'Neutral', 'Positive']]\n",
        "dfmerge1"
      ],
      "metadata": {
        "id": "orVbeSoUH5IK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = MinMaxScaler()\n",
        "\n",
        "df = pd.DataFrame(scaler.fit_transform(dfmerge1))\n",
        "df.columns = dfmerge1.columns\n",
        "df.index = dfmerge1.index\n",
        "df.head()"
      ],
      "metadata": {
        "id": "k_49Qs0iH4xP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=df.drop('Close',axis=1)\n",
        "X"
      ],
      "metadata": {
        "id": "hLwML9wBH_H0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y=df['Close']\n",
        "Y"
      ],
      "metadata": {
        "id": "nKO19-LNICdL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state = 0)\n",
        "x_train.shape"
      ],
      "metadata": {
        "id": "i66ZxyaEIGAL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train[:10]"
      ],
      "metadata": {
        "id": "yd9Z2NejICHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestRegressor()\n",
        "rf.fit(x_train, y_train)\n",
        "prediction=rf.predict(x_test)\n",
        "print(prediction[:10])\n",
        "print(y_test[:10])\n",
        "print('Mean Squared error: ',mean_squared_error(prediction,y_test))"
      ],
      "metadata": {
        "id": "0wlVbsQWId8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dtr = DecisionTreeRegressor()\n",
        "dtr.fit(x_train, y_train)\n",
        "predictions = dtr.predict(x_test)\n",
        "print(predictions[:10])\n",
        "print(y_test[:10])\n",
        "print('Mean Squared error: ',mean_squared_error(predictions,y_test))"
      ],
      "metadata": {
        "id": "VXZZquSdIhZS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adb = AdaBoostRegressor()\n",
        "adb.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "HZZ7Rmz0Il6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = adb.predict(x_test)\n",
        "print(mean_squared_error(predictions, y_test))"
      ],
      "metadata": {
        "id": "dirEE_K9IuxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gbm = lightgbm.LGBMRegressor()\n",
        "gbm.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "_iG5rYI7JDAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = gbm.predict(x_test)\n",
        "print(mean_squared_error(predictions, y_test))"
      ],
      "metadata": {
        "id": "bteionLAJHU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xgb = xgboost.XGBRegressor()\n",
        "xgb.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "4VLeLR4tJKmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = xgb.predict(x_test)\n",
        "print(mean_squared_error(predictions, y_test))"
      ],
      "metadata": {
        "id": "m04_dI1nJNaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df_merge[['Close','Subjectivity', 'Polarity', 'Compound', 'Negative', 'Neutral' ,'Positive']]\n",
        "df.head()"
      ],
      "metadata": {
        "id": "bfcWlalIE35x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "sc = MinMaxScaler()\n",
        "new_df = pd.DataFrame(sc.fit_transform(df))\n",
        "new_df.columns = df.columns\n",
        "new_df.index = df.index\n",
        "new_df.head()\n"
      ],
      "metadata": {
        "id": "j1OYUV_OE97C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spliting Data into Train and test"
      ],
      "metadata": {
        "id": "Pr7EDEhAFC5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = new_df.drop('Close', axis=1)\n",
        "y =new_df['Close']"
      ],
      "metadata": {
        "id": "_dXzUWwAFDnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.2, random_state = 0)\n",
        "X_train.shape , X_test.shape"
      ],
      "metadata": {
        "id": "RPGJBwcgFJAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def func_graph(results,names):\n",
        "    fig = plt.figure()\n",
        "    fig.suptitle('MSE value of all Algorithms Comparison')\n",
        "    ax = fig.add_subplot(111)\n",
        "    width = 0.5\n",
        "    bars=plt.bar(names,results, width, align='center')\n",
        "    ax.set_xticklabels(names)\n",
        "    for bar in bars:\n",
        "        yval = bar.get_height()\n",
        "        plt.text(bar.get_x(), yval +0.005, yval)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "vI-lBZ5HFOXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "def metric_calc(name,model,category, X_train, Y_train, X_test, Y_test):\n",
        "    if category =='TRAINING DATA' :\n",
        "        X_data= X_train\n",
        "        Y_data=Y_train\n",
        "    else :\n",
        "        X_data= X_test\n",
        "        Y_data=Y_test\n",
        "\n",
        "    model.fit(X_train, Y_train)\n",
        "    predictions = model.predict(X_data)\n",
        "    mse =round(metrics.mean_squared_error(predictions,Y_data),4)\n",
        "    print('For ', name, 'MSE-Value is ', mse)\n",
        "    return mse"
      ],
      "metadata": {
        "id": "cWmU92J7FR2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def func_modelling(i) :\n",
        "    count=0\n",
        "    count=count+1\n",
        "    X = X_train[i]\n",
        "    Y = Y_train\n",
        "    x_test = X_test[i]\n",
        "    seed = 7\n",
        "    # preparing models list\n",
        "    models = []\n",
        "    models.append(('Decision Tree',' DecisiontreeRegressor  ', DecisionTreeRegressor()))\n",
        "    models.append(('Random Forest',' RandomForestRegressor  ', RandomForestRegressor()))\n",
        "    models.append(('XG Boost',' XGBRegressor  ', xgboost.XGBRegressor()))\n",
        "    models.append(('LG Boost',' LGBMRegressor ', lightgbm.LGBMRegressor()))\n",
        "    models.append(('ADA Boost',' AdaBoostRegressor ', AdaBoostRegressor()))\n",
        "    results_train = []\n",
        "    results_test = []\n",
        "    names = []\n",
        "    scoring = 'MSE'\n",
        "\n",
        "    print('Metrics calcuated while TRANING the model')\n",
        "    for name,label, model in models:\n",
        "            cv_results_train=metric_calc(name,model,'TRAINING DATA',X,Y, x_test,Y_test)\n",
        "            results_train.append(cv_results_train)\n",
        "            names.append(name)\n",
        "    func_graph(results_train,names)\n",
        "\n",
        "    print('Evaluating the model on TESTING DATA')\n",
        "    for name,label, model in models:\n",
        "            cv_results_test=metric_calc(name,model,'TESTING DATA',X,Y, x_test,Y_test)\n",
        "            results_test.append(cv_results_test)\n",
        "            #names.append(name)\n",
        "    func_graph(results_test,names)"
      ],
      "metadata": {
        "id": "IQyd4r2vFV5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the model"
      ],
      "metadata": {
        "id": "oXHU1noyFYlg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "func_modelling(X_train.columns)"
      ],
      "metadata": {
        "id": "Ut0f4LcbFZYN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}